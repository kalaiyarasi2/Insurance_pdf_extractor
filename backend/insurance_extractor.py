"""
Enhanced Insurance Form Extractor with PyMuPDF + Tesseract
Features:
- Direct PDF text extraction with PyMuPDF
- Tesseract OCR for scanned content
- Layout-aware structure preservation
- Schema extraction with GPT-4
- User verification of extracted text
"""

import os
import json
import base64
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, asdict
from datetime import datetime
import re
from pathlib import Path
import subprocess
import sys
from io import BytesIO

try:
    import fitz  # PyMuPDF
    from PIL import Image
    import pytesseract
    from openai import OpenAI
except ImportError:
    print("Installing required packages...")
    packages = ["pymupdf", "pytesseract", "Pillow", "openai"]
    for pkg in packages:
        subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])
    import fitz
    from PIL import Image
    import pytesseract
    from openai import OpenAI


@dataclass
class PageExtraction:
    """Data for a single page"""
    page_number: int
    image_path: str
    raw_text: str
    orientation: str  # 'portrait' or 'landscape'
    is_scanned: bool
    confidence: float


@dataclass
class InsuranceClaim:
    """Insurance Claim Data Structure"""
    employee_name: Optional[str] = None
    claim_number: Optional[str] = None
    injury_date_time: Optional[str] = None
    claim_year: Optional[int] = None
    status: Optional[str] = None
    injury_description: Optional[str] = None
    body_part: Optional[str] = None
    injury_type: Optional[str] = None
    claim_class: Optional[str] = None
    medical_paid: Optional[float] = None
    medical_reserve: Optional[float] = None
    indemnity_paid: Optional[float] = None
    indemnity_reserve: Optional[float] = None
    expense_paid: Optional[float] = None
    expense_reserve: Optional[float] = None
    recovery: Optional[float] = None
    deductible: Optional[float] = None
    total_incurred: Optional[float] = None
    confidence_score: Optional[float] = None
    extraction_metadata: Optional[Dict] = None


@dataclass
class LossRunReport:
    """Complete Loss Run Report with multiple claims"""
    policy_number: Optional[str] = None
    insured_name: Optional[str] = None
    report_date: Optional[str] = None
    policy_period: Optional[str] = None
    claims: Optional[List[InsuranceClaim]] = None
    extraction_metadata: Optional[Dict] = None
    
    def __post_init__(self):
        if self.claims is None:
            self.claims = []
    
    @property
    def total_claims(self) -> int:
        """Return total number of claims"""
        return len(self.claims) if self.claims else 0
    
    @property
    def total_incurred_all(self) -> float:
        """Return sum of all incurred amounts"""
        if not self.claims:
            return 0.0
        return sum(claim.total_incurred or 0 for claim in self.claims)


class EnhancedInsuranceExtractor:
    """Enhanced extractor with layout awareness and verification"""
    
    def __init__(self, api_key: Optional[str] = None, output_dir: Optional[str] = None):
        """Initialize with OpenAI API"""
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key)
            print("‚úì GPT-4 Vision API initialized")
        else:
            raise ValueError("OPENAI_API_KEY is required for enhanced extraction")
        
        self.output_dir = Path(output_dir) if output_dir else Path("outputs")
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def extract_text_from_pdf(self, pdf_path: str) -> Tuple[str, List[Dict]]:
        """
        Extract text from PDF using detection and appropriate extraction method.
        """
        from pdf_detector import PDFDetector
        
        try:
            print(f"üîç Detecting PDF type...")
            detector = PDFDetector(pdf_path)
            is_scanned = detector.is_scanned()
            
            if is_scanned:
                print(f"üì∏ SCANNED PDF DETECTED: Using Tesseract OCR fallback")
                from ocr_text import OCRPDFExtractor
                ocr_extractor = OCRPDFExtractor(pdf_path)
                return ocr_extractor.extract()
            else:
                print(f"üìÑ DIGITAL PDF DETECTED: Using Hybrid Extraction (pdfplumber + pymupdf fallback)")
                from pdf_plumber import extract_pdf_hybrid
                # Hybrid extraction returns (text, metadata, info)
                text, metadata, info = extract_pdf_hybrid(pdf_path)
                
                if info.get('fallback_used'):
                    print(f"   ‚ÑπÔ∏è Hybrid Extraction recovered {len(info.get('recovered_claims', []))} claims using Smart Append")
                    
                return text, metadata
                
        except Exception as e:
            print(f"‚ö†Ô∏è Detection/Extraction error: {e}")
            print(f"   Falling back to standard pdfplumber...")
            from pdf_plumber import extract_pdf_with_pdfplumber as external_extract
            return external_extract(pdf_path)
    
    
    def _detect_claim_numbers_ai(self, text: str) -> Dict:
        """
        Use AI to detect ALL claim numbers in the document
        NO HARDCODED PATTERNS - AI figures it out!
        """
        print(f"\nüîç Using AI to detect claim number patterns...")
        
        prompt = f"""You are an expert at analyzing insurance documents and identifying claim numbers.

Your task: Analyze this insurance document and IDENTIFY ALL UNIQUE CLAIM NUMBERS.

=== CRITICAL DISTINCTION: POLICY NUMBER vs CLAIM NUMBER ===

POLICY NUMBERS:
- Identify an entire insurance policy (covers an insured for a time period)
- Example: "SWC1364773" or "TWC4172502"
- Typically appear in a consistent location on every page
- Multiple different claims can belong to the SAME policy number
- Look for field labels like "Policy Number", "Policy #", "Pol #"

CLAIM NUMBERS:
- Identify a SINGLE claim/incident (one employee's injury)
- Each claim is UNIQUE and appears only once in the document
- Examples: "CLAIM-123", "ABC-456", "2024-001"
- Often shown after "Claim #", "Claim No", or similar labels
- Can be simple numeric format OR prefixed format

GOLDEN RULE: If you see the SAME number appear as a header on multiple claim sections, it's a POLICY number, NOT a claim number.
           If you see a DIFFERENT number for each claim/injury, those are CLAIM numbers.

- "Converted #" field (e.g., [CLAIM_NUMBER]) = ACTUAL claim number (unique per claim)
- ‚ùå DO NOT extract SWC/TWC numbers as claim numbers!
- ‚úÖ DO extract values after "Converted #" as claim numbers!

IMPORTANT INSTRUCTIONS:
1. **Literal Extraction Only**:
   - Extract the claim number EXACTLY as it is written in the document.
   - **NEVER** invent, assume, or append suffixes (like "-01", "-02") if they aren't explicitly typed in the text.
   - **Berkshire Homestates/Redwood Blacklist**: EXPLICITLY IGNORE any strings starting with `CRWC`. These are Policy Numbers, NOT claim numbers. 
   - **Homestates Format**: Claim numbers are typically 8-digit integers (e.g., `44070643`).
   - If the document says `ABC123`, result must be `ABC123`. Do NOT add `-01`.

2. **The Header vs. Row Separation**:
   - **Policy Numbers**: Usually in Headers (labeled "Policy #", "Policy Number"). These are **EXCLUSIONS**.
   - **Claim Numbers**: Found within data rows, paired with "Claimant Name" and "Date of Incident".

3. **Strict Validation**:
   - A string is ONLY a claim number if it is paired with actual incident data (Name, Date).
   - **DO NOT** create a claim entry if the only number you find is a `CRWC` policy number.

3. STRICT EXCLUSIONS (DO NOT LIST AS CLAIM NUMBERS):
   - Policy numbers (even if they look like claim numbers)
   - Page numbers
   - Dates
   - Dollar amounts
   - Employee IDs
   - Report IDs

=== SELF-VALIDATION INSTRUCTIONS ===

After detecting claim numbers, perform these checks:

1. **Uniqueness Test**: 
   - Count how many times each detected number appears in the document
   - If a number appears on EVERY page or for MULTIPLE different employees ‚Üí It's a POLICY number, NOT a claim number
   
2. **Pattern Analysis**:
   - Analyze the format of detected numbers
   - If all numbers follow the same prefix pattern (e.g., all start with "SWC") ‚Üí Likely policy numbers
   - If numbers are diverse in format ‚Üí Likely claim numbers
   
3. **Context Validation**:
   - Check what label appears before each number
   - "Policy #", "Policy Number" ‚Üí EXCLUDE
   - "Claim #", "Claim Number", "Converted #" ‚Üí INCLUDE
   
4. **Cross-Reference Check**:
   - Compare detected numbers against employee names
   - Each unique employee should have a unique claim number
   - If same number appears for multiple employees ‚Üí POLICY number

For each claim number found, note:
   - The exact format/pattern it follows
   - Where it appears in the document
   - How confident you are it's a claim number (0.0-1.0)
   - Validation results from the checks above

Return a JSON object with this structure:

{{
  "claim_numbers": [
    {{
      "claim_number": "20825",
      "pattern_description": "Follows 'Claim#' label",
      "first_occurrence": "near line 45",
      "confidence": 0.95,
      "validation_passed": true,
      "uniqueness_score": 1.0,
      "context_label": "Claim#"
    }}
  ],
  "rejected_numbers": [
    {{
      "number": "SWC1364773",
      "reason": "Appears for multiple employees - likely policy number",
      "context_label": "Policy Number"
    }}
  ],
  "detected_patterns": [
    {{
      "pattern_name": "FCBIF format",
      "pattern_description": "Claim# followed by digits",
      "example": "Claim# 20825",
      "count": 7
    }}
  ],
  "total_unique_claims": 7,
  "confidence": 0.92
}}

DOCUMENT TEXT (COMPLETE):
{text}

Return ONLY the JSON. No explanations. Ensure you catch EVERY claim number, especially those on later pages. Scan the ENTIRE text length.
"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": prompt
                }],
                response_format={"type": "json_object"},
                max_tokens=8000,
                temperature=0.0
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # Extract claim numbers
            claim_numbers = [c["claim_number"] for c in result.get("claim_numbers", [])]
            patterns = result.get("detected_patterns", [])
            
            print(f"‚úì AI detected {len(claim_numbers)} unique claim numbers")
            for pattern in patterns:
                print(f"  - {pattern['pattern_name']}: {pattern['count']} claims")
            
            return result
            
        except Exception as e:
            print(f"‚ùå Error in AI claim detection: {e}")
            import traceback
            traceback.print_exc()
            return {
                "claim_numbers": [],
                "detected_patterns": [],
                "total_unique_claims": 0,
                "confidence": 0.0
            }
    
        image.save(buffered, format="PNG")
        img_base64 = base64.b64encode(buffered.getvalue()).decode()
        
        prompt = """You are an expert OCR system that preserves document layout and structure.

Your task: Extract ALL text from this document while preserving its EXACT layout.

‚ö†Ô∏è CRITICAL: If this is a BLANK PAGE or ERROR MESSAGE, indicate that clearly in your response.

CRITICAL REQUIREMENTS:
1. **Preserve Tables**: Keep rows and columns aligned using spaces or tabs
2. **Maintain Spacing**: Keep vertical spacing between sections
3. **Column Alignment**: If document has multiple columns, keep them separate
4. **Headers & Labels**: Clearly show all field labels and their values
5. **Numbers**: Extract all numbers with exact precision (decimals, commas)
6. **Handle Scans**: This may be a scanned document - extract carefully
7. **Orientation**: Document may be landscape or portrait - extract accordingly
8. **Blank Pages**: If page appears blank or contains only an error message, indicate this

EXTRACT EVERYTHING including:
- All headers and titles
- Field labels and their values
- Table contents (all rows and columns)
- Financial amounts
- Dates and times
- Names and identifiers
- Any footnotes or small text

IF THIS PAGE IS BLANK OR CONTAINS ONLY AN ERROR:
- Type: [BLANK PAGE] or [ERROR MESSAGE]
- Description of what you see

FORMAT YOUR RESPONSE AS:

```
[EXTRACTED TEXT - LAYOUT PRESERVED]
<paste the full text here maintaining layout>

[DOCUMENT ANALYSIS]
- Is Scanned: <yes/no>
- Quality: <excellent/good/fair/poor>
- Confidence: <0.0-1.0>
- Layout Type: <table/form/mixed/blank>
- Orientation: <portrait/landscape/unknown>
- Page Status: <content/blank/error>
```

IMPORTANT: 
- Do NOT summarize. Extract the COMPLETE text exactly as it appears.
- If page is blank or shows an error, still report the confidence as 0.0"""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{img_base64}"
                            }
                        }
                    ]
                }],
                max_tokens=8000,
                temperature=0.0  # Zero temperature for exact extraction
            )
            
            response_text = response.choices[0].message.content
            
            # Parse response
            extracted_text = ""
            is_scanned = False
            confidence = 0.9
            page_status = "content"
            
            # Extract the text section
            if "[EXTRACTED TEXT - LAYOUT PRESERVED]" in response_text:
                parts = response_text.split("[DOCUMENT ANALYSIS]")
                text_section = parts[0].replace("[EXTRACTED TEXT - LAYOUT PRESERVED]", "").strip()
                extracted_text = text_section.strip('`').strip()
                
                # Parse analysis section
                if len(parts) > 1:
                    analysis = parts[1]
                    if "Is Scanned: yes" in analysis.lower():
                        is_scanned = True
                    
                    # Extract confidence score
                    conf_match = re.search(r'Confidence:\s*([\d\.]+)', analysis)
                    if conf_match:
                        confidence = float(conf_match.group(1))
                    
                    # Check page status
                    if "[BLANK PAGE]" in extracted_text or "[ERROR MESSAGE]" in extracted_text:
                        page_status = "blank"
                        confidence = 0.0
                        extracted_text = "[BLANK PAGE - No extractable content]"
                    elif "Page Status: blank" in analysis.lower():
                        page_status = "blank"
                        confidence = 0.0
                        extracted_text = "[BLANK PAGE - No extractable content]"
            else:
                # Fallback: use entire response
                extracted_text = response_text
            
            print(f"‚úì Extracted {len(extracted_text)} characters")
            print(f"  - Scanned: {is_scanned}")
            print(f"  - Confidence: {confidence:.2f}")
            print(f"  - Status: {page_status}")
            
            return extracted_text, is_scanned, confidence
            
        except Exception as e:
            print(f"‚ùå Error extracting text: {e}")
            return "", False, 0.0
    
    def _chunk_text_dynamically(self, text: str, max_tokens: int = 6000) -> List[Dict]:
        """
        Use AI to intelligently split large documents into chunks.
        
        AI determines:
        - Natural boundaries (claim sections, page breaks)
        - Optimal overlap size to preserve context
        - Which sections can be safely split vs must stay together
        
        Returns: List of chunks with metadata
        """
        # If text is small enough, return as single chunk
        estimated_tokens = len(text) // 4  # Rough estimate: 1 token ‚âà 4 chars
        if estimated_tokens <= max_tokens:
            return [{
                "chunk_id": 0,
                "text": text,
                "start_pos": 0,
                "end_pos": len(text),
                "strategy": "no_chunking_needed"
            }]
        
        print(f"\nüìä Document is large ({estimated_tokens} est. tokens). Using AI to determine chunking strategy...")
        
        # Sample beginning and end for AI analysis
        sample_text = text[:2000] + "\n...\n" + text[-1000:]
        
        prompt = f"""Analyze this insurance document and suggest optimal split points for processing.

Document length: {len(text)} characters (~{estimated_tokens} tokens)
Target chunk size: ~{max_tokens} tokens

Your task:
1. Identify natural boundaries (claim sections, page breaks, table boundaries)
2. Suggest split points that preserve complete claim information
3. Determine overlap needed between chunks to maintain context

IMPORTANT:
- Each chunk should contain COMPLETE claims (don't split a claim across chunks)
- Look for patterns like "PAGE X", "Claim#", "Employee Name:" that indicate boundaries
- Suggest overlap to ensure no data is lost between chunks

Return JSON:
{{
  "suggested_splits": [
    {{"position": 15000, "reason": "After claim section ends", "overlap_before": 300}},
    {{"position": 32000, "reason": "Page break detected", "overlap_before": 200}}
  ],
  "optimal_overlap": 300,
  "chunking_strategy": "claim-boundary-aware",
  "confidence": 0.95
}}

If no clear boundaries are found, suggest splitting at paragraph breaks with generous overlap.

DOCUMENT SAMPLE:
{sample_text}
"""
        
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                max_tokens=1500,
                temperature=0.0
            )
            
            chunking_plan = json.loads(response.choices[0].message.content)
            splits = chunking_plan.get("suggested_splits", [])
            default_overlap = chunking_plan.get("optimal_overlap", 300)
            
            print(f"   ‚úì AI suggested {len(splits)} split points")
            print(f"   ‚úì Strategy: {chunking_plan.get('chunking_strategy', 'adaptive')}")
            
            # Build chunks based on AI suggestions
            chunks = []
            current_pos = 0
            
            for idx, split in enumerate(splits):
                split_pos = split.get("position", 0)
                overlap = split.get("overlap_before", default_overlap)
                
                # Ensure split position is within bounds
                if split_pos > len(text):
                    split_pos = len(text)
                
                # Create chunk with overlap
                chunk_start = max(0, current_pos - overlap if idx > 0 else 0)
                chunk_end = split_pos
                
                chunks.append({
                    "chunk_id": idx,
                    "text": text[chunk_start:chunk_end],
                    "start_pos": chunk_start,
                    "end_pos": chunk_end,
                    "overlap": overlap if idx > 0 else 0,
                    "reason": split.get("reason", "AI-determined boundary")
                })
                
                current_pos = split_pos
            
            # Add final chunk
            if current_pos < len(text):
                chunks.append({
                    "chunk_id": len(chunks),
                    "text": text[max(0, current_pos - default_overlap):],
                    "start_pos": max(0, current_pos - default_overlap),
                    "end_pos": len(text),
                    "overlap": default_overlap,
                    "reason": "Final section"
                })
            
            return chunks
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è AI chunking failed: {e}")
            print(f"   Falling back to simple chunking...")
            
            # Fallback: Simple chunking with fixed overlap
            chunks = []
            chunk_size = max_tokens * 4  # Convert tokens to chars
            overlap = 500
            current_pos = 0
            chunk_id = 0
            
            while current_pos < len(text):
                chunk_end = min(current_pos + chunk_size, len(text))
                chunk_start = max(0, current_pos - overlap if chunk_id > 0 else 0)
                
                chunks.append({
                    "chunk_id": chunk_id,
                    "text": text[chunk_start:chunk_end],
                    "start_pos": chunk_start,
                    "end_pos": chunk_end,
                    "overlap": overlap if chunk_id > 0 else 0,
                    "strategy": "fallback_fixed_size"
                })
                
                current_pos = chunk_end
                chunk_id += 1
            
            return chunks
    
    def extract_schema_from_text(self, all_text: str, target_claim_number: Optional[str] = None) -> Dict:
        """
        Extract structured schema from verified text
        NOW SUPPORTS MULTIPLE CLAIMS!
        """
        print(f"\nüéØ Extracting schema from text...")
        
        # Decide whether to extract all claims or just one
        if target_claim_number:
            print(f"   Target: Claim #{target_claim_number} only")
            return self._extract_single_claim(all_text, target_claim_number)
        else:
            print(f"   Target: ALL claims in document")
            return self._extract_all_claims(all_text)
    
    def _analyze_document_format(self, text: str) -> Dict:
        """
        STAGE 1: Analyze document structure and format
        Let GPT-4 figure out how the data is organized
        """
        print(f"\nüîç STAGE 1: Analyzing document format...")
        
        prompt = f"""You are analyzing an insurance loss run report to understand its structure.

Your task: Describe HOW the data is organized in this document so we can extract it accurately.

Answer these questions:
1. What is the insurance company/carrier name?
2. How are claims organized? (one per row, multi-row per claim, one per page?)
3. How are financial amounts presented?
   - Simple columns (Ind Paid, Med Paid, etc.)?
   - Complex multi-row tables (Incurred/Paid/Reserves rows)?
4. IMPORTANT: Determine the EXACT row order for financial data (e.g., Row 1: Reserves, Row 2: Payments, Row 3: Incurred).
5. How are the numeric columns ordered? (e.g., Med, Ind, LAE/Exp, Total)
6. Are there specific labels that anchor the rows? (e.g., "Payments", "Payments:", "Reserves")

Return JSON:
{{
  "insurer": "company name",
  "format_type": "simple_columns" or "complex_multi_row" or "mixed",
  "claim_layout": "one_per_row" or "multi_row_per_claim" or "one_per_page",
  "financial_mapping": {{
    "row_1": "label",
    "row_2": "label",
    "row_3": "label",
    "column_order": ["field1", "field2", "..."],
    "dynamic_instruction": "A custom extraction rule you generate specifically for this layout"
  }},
  "special_notes": "any quirks or unusual formatting",
  "confidence": 0.0-1.0
}}

DOCUMENT TEXT (first 8000 chars):
{text[:8000]}

Return ONLY the JSON. Ensure the dynamic_instruction is highly technical and specific about which line to read for 'Paid' vs 'Reserves'."""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                max_tokens=1500,
                temperature=0.0
            )
            
            format_info = json.loads(response.choices[0].message.content)
            
            print(f"   ‚úì Format detected: {format_info.get('format_type', 'unknown')}")
            print(f"   ‚úì Insurer: {format_info.get('insurer', 'unknown')}")
            print(f"   ‚úì Claim layout: {format_info.get('claim_layout', 'unknown')}")
            print(f"   ‚úì Confidence: {format_info.get('confidence', 0.0):.2%}")
            
            return format_info
            
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Format analysis failed: {e}")
            return {
                "format_type": "unknown",
                "confidence": 0.0
            }
    
    def _extract_all_claims(self, all_text: str) -> Dict:
        """
        UNIVERSAL EXTRACTION: Works with ANY format
        Uses a three-stage approach:
        1. Pre-Discovery: Detect all valid Claim IDs (Master List)
        2. Format Analysis: Understand the layout
        3. Constrained Extraction: Extract only those IDs
        """
        # STAGE 0: Pre-Discovery (Master List)
        detected_claims_info = self._detect_claim_numbers_ai(all_text)
        master_claim_list = [c["claim_number"] for c in detected_claims_info.get("claim_numbers", [])]
        
        if not master_claim_list:
            print("   ‚ö†Ô∏è No unique claim numbers discovered. Falling back to layout-only extraction.")
            master_list_str = "No pre-detected list available. Detect claims dynamically."
        else:
            print(f"   ‚úì Pre-discovered {len(master_claim_list)} valid claim IDs.")
            master_list_str = ", ".join(master_claim_list)
            
        # STAGE 1: Analyze document format
        format_info = self._analyze_document_format(all_text)
        
        # STAGE 2: Build adaptive extraction prompt
        print(f"\nüéØ STAGE 2: Extracting claims using constrained adaptive prompt...")
        
        # Build format-specific instructions
        format_type = format_info.get('format_type', 'unknown')
        financial_mapping = format_info.get('financial_mapping', {})
        dynamic_rules = financial_mapping.get('dynamic_instruction', 'Extract all financial fields carefully.')
        
        # Injected Accuracy Constraints
        accuracy_constraints = f"""
=== ACCURACY CONSTRAINTS (MANDATORY) ===
1. MASTER CLAIM LIST: {master_list_str}
2. üõë ZERO-PHANTOM POLICY: Extract ONLY claims from the MASTER CLAIM LIST above. 
   - NEVER include placeholder names like 'John Smith', 'Jane Doe', or 'Jane Smith'. 
   - These are calibration examples and NOT real data in this document.
   - If a claim ID is not in the list, DO NOT extract it.
3. üõë FIELD INTEGRITY: Do NOT swap Medical and Indemnity columns. Check headers for each row.
4. üõë CURRENCY: Remove all symbols ($, ,) and return numbers as floats.
"""
        
        if format_type == 'complex_multi_row':
            financial_instructions = f"""
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üî¥ AMTRUST CALIBRATION (Mandatory) üî¥
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
AmTrust PDFs use a strict 3-Row by 4-Column structure. Use these examples to CALIBRATE your mapping:

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üî¥ AMTRUST CALIBRATION (Mandatory) üî¥
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
CALIBRATION 1: Duarte Milian (3631112) -> MED=966, INDEM=2,926, EXP=173.
CALIBRATION 2: Johnson Linda (3543022) -> EXPENSE PAID=1,427.
CALIBRATION 3: Boyce Michael (3675064) -> MEDICAL RESERVE=6,862. (Sum with Paid 26,303 = 33,165).
CALIBRATION 4: Watson Glenn (3674444) -> EXPENSE RESERVE=0. (Note: 9,924 is often Expenses, but mapping varies).

‚ö†Ô∏è AMTRUST COLUMN ORDER: 1. MEDICAL, 2. INDEMNITY, 3. EXPENSE / LAE.

‚ö†Ô∏è AMTRUST INDEMNITY CALCULATION:
- If you see both "TD" (Temporary Disability) and "PD" (Permanent Disability) for a single claim, YOU MUST SUM THEM.
- medical_paid = Medical Paid
- medical_reserve = Medical Outstanding
- indemnity_paid = TD Paid + PD Paid
- indemnity_reserve = TD Outstanding + PD Outstanding
- expense_paid = Expense Paid
- expense_reserve = Expense Outstanding

‚ö†Ô∏è STATUS MAPPING (Strict):
- "C" -> "Closed"
- "O" -> "Open"
- "R", "RC", "REOP" -> "Reopened"

‚ö†Ô∏è MATH CHECKSUM:
Paid + Reserve == Incurred (For each category).
Sum of (M, I, E) = Total.
If the math doesn't match perfectly, you have swapped Columns or missed PD/TD summation!
"""
        elif format_type == 'simple_columns':
            financial_instructions = """
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ SIMPLE COLUMNAR FORMAT DETECTED ‚úÖ
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

This format has clearly labeled columns. Extract values directly:
- Look for columns: Ind Paid, Ind Resv, Med Paid, Med Resv, Exp Paid, Exp Resv, Total Inc
- Each claim is one row
- Read values directly from the columns
- NO complex calculations needed

MAPPING:
- medical_paid = "Med Paid" column
- medical_reserve = "Med Resv" column
- indemnity_paid = "Ind Paid" column
- indemnity_reserve = "Ind Resv" column
- expense_paid = "Exp Paid" column
- expense_reserve = "Exp Resv" column
- recovery = "Recov" column
- total_incurred = "Total Inc" column
"""
        else:
            financial_instructions = """
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚ö†Ô∏è UNKNOWN/MIXED FORMAT DETECTED ‚ö†Ô∏è
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Carefully analyze each claim's structure and extract accordingly.
Look for patterns in how financial data is presented.
Validate your extractions by checking if Paid + Reserve = Incurred.
"""
        
        # Build the complete extraction prompt
        prompt = f"""You are an expert at extracting structured data from insurance loss run reports.

DOCUMENT FORMAT ANALYSIS:
{json.dumps(format_info, indent=2)}

{financial_instructions}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìã EXTRACTION TASK
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

{accuracy_constraints}

Extract EVERY SINGLE CLAIM from this document.

Return JSON:
{{
  "policy_number": "string or null",
  "insured_name": "string or null",
  "report_date": "YYYY-MM-DD or null",
  "policy_period": "string or null",
  "claims": [
    {{
      "employee_name": "full name",
      "claim_number": "claim number",
      "injury_date_time": "YYYY-MM-DD",
      "claim_year": 2020,
      "status": "Open or Closed or Reopened",
      "injury_description": "description",
      "body_part": "body part or null",
      "injury_type": "MED or COMP",
      "claim_class": "class code or null",
      "medical_paid": "string (e.g. '1,973.00')",
      "medical_reserve": "string",
      "indemnity_paid": "string",
      "indemnity_reserve": "string",
      "expense_paid": "string",
      "expense_reserve": "string",
      "recovery": "string",
      "deductible": "string",
      "total_incurred": "string"
    }}
  ]
}}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
ÔøΩ GENERAL EXTRACTION RULES (Apply to ALL formats)
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ


1. CLAIM NUMBER vs POLICY NUMBER - CRITICAL DISTINCTION
   
   ‚ö†Ô∏è MOST IMPORTANT RULE: DO NOT confuse policy numbers with claim numbers!
   
   **POLICY NUMBERS:**
   - Identify an entire insurance policy (covers multiple claims over a time period)
   - Examples: SWC1364773, TWC4172502, ZAWCI9740001, Policy #12345
   - Appear REPEATEDLY throughout the document (same number for multiple claims)
   - Found in fields labeled: "Policy Number", "Policy #", "Pol #", "Policy No"
   - ‚ùå DO NOT use policy numbers as claim numbers!
   
   **CLAIM NUMBERS:**
   - Identify a SINGLE claim/incident (one employee's injury)
   - Each claim number is UNIQUE - appears only ONCE in the document
   - Examples: 3510012, 20825, DEL22003452, Claim#20677
   - Found in fields labeled: "Claim #", "Claim No", "Claim Number", "Converted #"
   
   **FORMAT-SPECIFIC GUIDANCE:**
   
   A. **AmTrust Format:**
      - Policy Number: SWC1364773 or TWC4172502 (appears at top of each claim)
      - Claim Number: Look for "Converted #" field (e.g., 3510012, 3543022)
      - ‚úÖ USE: The "Converted #" value
      - ‚ùå IGNORE: The "Policy Number" value
   
   B. **FCBIF Format:**
      - Claim Number: Look for "Claim#" followed by number (e.g., Claim# 20825)
      - ‚úÖ USE: The number after "Claim#"
   
   C. **DeliverRe Format:**
      - Claim Number: Starts with "DEL" (e.g., DEL22003452)
      - ‚úÖ USE: The DEL-prefixed number

   D. **Strict Identification Rules (CRITICAL):**
      - **NO SUFFIX INVENTION**: Do NOT append characters to a number unless you see them in the raw text.
      - **CRWC Blacklist**: Numbers starting with `CRWC` are POLICY NUMBERS. Never extract them as claims. 
      - **Berkshire Homestates**: Claim numbers are 8-digit integers found next to the name (e.g., `44096049`).
      - **BiBERK (N9WC)**: These *do* have literal suffixes in the text (e.g., `-001`). Extract them exactly.
      - **Literal Match**: If the row says `44062808`, yours must be `44062808`. Do NOT add `-01`, `-02` etc.
   
   **VALIDATION:**
   - If you see the SAME number appearing for multiple different employees ‚Üí It's a POLICY number, NOT a claim number
   - If each employee has a DIFFERENT number ‚Üí Those are CLAIM numbers ‚úì
   
   **GOLDEN RULE:** When in doubt, look for:
   - "Claim #:", "Claim No:", "Claim Number:", "Converted #" ‚Üí These introduce CLAIM numbers
   - "Policy Number", "Policy #", "Pol #" ‚Üí These introduce POLICY numbers (ignore these!)


2. EMPLOYEE NAME
   - Look for "Claimant:", "Employee Name:", or similar labels
   - Extract full name as shown

3. DATES - CRITICAL: USE DATE OF LOSS (DOL)
   - ALWAYS use "DOL" or "Date of Loss" for injury_date_time
   - DO NOT use "Date Rcvd" or "First Aware" - these are reporting dates
   - Convert all dates to YYYY-MM-DD format
   - Look for: "DOL:", "Loss Date:", "Injury Date:", "Occ Date:", "Accident Date:"

4. STATUS
   - C or Closed ‚Üí "Closed"
   - O or Open ‚Üí "Open"
   - REOP or Reopened or R or RC ‚Üí "Reopened"

5. INJURY TYPE
   - Medical or MED or MEDI or "Medical Only" or "Record Only" ‚Üí "MED"
   - Indemnity or COMP or Compensation or TTD or TPD or PPD ‚Üí "COMP"

6. BODY PART
   - Extract from "Nature of Injury", "Body Part", "Part Injured" fields
   - If not found, use null

7. INJURY DESCRIPTION
   - Look for "Nature of Injury:", "Cause of Injury:", "Loss Description:", "Accident Description:"
   - Extract the description text

8. CLAIM CLASS
   - Look for "Class Code:", "Class:", "Class Cd"
   - Extract the code (e.g., "7721", "7231")
   - If not found, use null

9. NUMBERS
   - Remove all $ signs and commas
   - Convert to decimal numbers
   - "$51,068.57" ‚Üí 51068.57

10. NULL VALUES
    - Use null for truly missing data
    - Use 0.0 for financial fields that are zero

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ VALIDATION CHECKLIST FOR EACH CLAIM
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Before adding a claim to your JSON, verify:

‚úì claim_number is extracted
‚úì employee_name is extracted
‚úì injury_date_time is in YYYY-MM-DD format
‚úì status is "Open", "Closed", or "Reopened"
‚úì All financial values are numbers (not strings)
‚úì Financial calculations balance (Paid + Reserve ‚âà Incurred)

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üìÑ TEXT TO ANALYZE
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

{all_text}

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
üéØ YOUR RESPONSE
‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ

Return ONLY the JSON object. No explanations. No markdown. Just the JSON.

Extract ALL claims. Do not skip any claim.

‚ö†Ô∏è CRITICAL REMINDER:
- This document may have MULTIPLE POLICY PERIODS or MULTIPLE POLICY SECTIONS
- You MUST scan the ENTIRE document from beginning to end
- Extract claims from ALL policy sections, NOT just the first one
- Continue extracting until you reach the end of the document
- Do NOT stop extraction after finding the first policy section totals

Follow the format-specific instructions above. Validate your extractions."""

        # Step 1: Initial Extraction Attempt
        data = {"claims": []}
        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": prompt
                }],
                response_format={"type": "json_object"},
                max_tokens=8000,
                temperature=0.0
            )
            
            response_text = response.choices[0].message.content
            initial_data = json.loads(response_text)
            
            # Check consistency
            if "claims" in initial_data:
                data = initial_data
            elif isinstance(initial_data, dict):
                # Maybe it returned a single object instead of a list
                data = {"claims": [initial_data]}
            
            # Step 1.5: Mathematical Self-Correction Loop
            failed_math_claims = [c.get("claim_number") for c in data.get("claims", []) if not c.get("math_valid", True)]
            
            if failed_math_claims:
                print(f"   ‚ö†Ô∏è  MATH VALIDATION FAILED for {len(failed_math_claims)} claim(s). Triggering self-correction...")
                print(f"   Failed IDs: {', '.join(str(c) for c in failed_math_claims)}")
                
                # Use a smaller batch size for correction to ensure focus
                correction_batch_size = 3
                for i in range(0, len(failed_math_claims), correction_batch_size):
                    batch = failed_math_claims[i:i + correction_batch_size]
                    print(f"   üîÑ Correction Batch {i//correction_batch_size + 1}: {', '.join(str(c) for c in batch)}")
                    
                    try:
                        correction_data = self._extract_missing_claims_by_number(all_text, data, batch, is_correction=True)
                        if correction_data and "claims" in correction_data:
                            # The post-processing logic in _extract_missing_claims_by_number will handle merging
                            # We just need to ensure the updated claims are added/replaced
                            for updated_claim in correction_data["claims"]:
                                # Data will be updated via deduplication in next post_process
                                data["claims"].append(updated_claim)
                    except Exception as e:
                        print(f"      ‚ö†Ô∏è Correction attempt failed: {e}")
                
                # Re-run post-processing to merge corrected claims
                data = self._post_process_claims(data)
                
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Initial extraction or correction failed: {e}")
            data = {"claims": []}

        # Step 2: Verification & Recovery (ALWAYS RUNS)
        try:
            # VALIDATION CHECK: Use AI to detect claim numbers
            detected_claims_info = self._detect_claim_numbers_ai(all_text)
            claims_in_text = detected_claims_info.get('total_unique_claims', 0)
            claims_extracted = len(data.get("claims", []))
            
            if claims_in_text > claims_extracted:
                print(f"\n   ‚ö†Ô∏è  INCOMPLETE EXTRACTION DETECTED")
                print(f"   Claims detected by AI: {claims_in_text}")
                print(f"   Claims extracted: {claims_extracted}")
                print(f"   Missing: {claims_in_text - claims_extracted}")
                
                # Get list of missing claim numbers
                detected_numbers = [c["claim_number"] for c in detected_claims_info.get("claim_numbers", [])]
                extracted_numbers = [c.get("claim_number") for c in data.get("claims", [])]
                missing_numbers = list(set(detected_numbers) - set(extracted_numbers))
                
                if missing_numbers:
                    print(f"   Attempting to extract {len(missing_numbers)} missing claims in batches...")
                        
                    # Process in batches of 5 to avoid timeouts/limits
                    batch_size = 5
                    for i in range(0, len(missing_numbers), batch_size):
                        batch = missing_numbers[i:i + batch_size]
                        print(f"   üîÑ Recovery Batch {i//batch_size + 1}: {', '.join(batch)}")
                            
                        # RETRY LOGIC for each batch
                        max_retries = 2
                        for attempt in range(max_retries):
                            try:
                                retry_data = self._extract_missing_claims_by_number(all_text, data, batch)
                                if retry_data and "claims" in retry_data:
                                    new_claims = retry_data["claims"]
                                    if new_claims:
                                        # Check for math validity in this batch too
                                        failed_in_batch = [c.get("claim_number") for c in new_claims if not c.get("math_valid", True)]
                                        if failed_in_batch and attempt < max_retries - 1:
                                            print(f"      ‚ö†Ô∏è  Math fail in recovery batch. Retrying correction for: {', '.join(str(c) for c in failed_in_batch)}")
                                            correction_data = self._extract_missing_claims_by_number(all_text, data, failed_in_batch, is_correction=True)
                                            if correction_data and "claims" in correction_data:
                                                # Replace failed claims with corrected ones
                                                valid_new_claims = [c for c in new_claims if str(c.get("claim_number")) not in [str(x) for x in failed_in_batch]]
                                                valid_new_claims.extend(correction_data["claims"])
                                                new_claims = valid_new_claims
                                        
                                        data["claims"].extend(new_claims)
                                        print(f"      ‚úì Retrieved {len(new_claims)} claims in this batch")
                                        break # Success
                            except Exception as e:
                                print(f"      ‚ö†Ô∏è  Recovery batch attempt {attempt+1} failed: {e}")
                                    
                        # Final merge check
                    data = self._post_process_claims(data)
                final_count = len(data.get("claims", []))
                print(f"   ‚úì Recovery complete. Final count: {final_count}/{claims_in_text}")
            else:
                print(f"   ‚úì All claims accounted for ({claims_in_text} total)")
                # Unconditionally run post-processing even if no claims were missing
                # to ensure claim_year and other normalizations are applied.
                data = self._post_process_claims(data)
                
        except Exception as e:
            print(f"   ‚ùå Error in recovery phase: {e}")
            import traceback
            traceback.print_exc()

        return data
            

    
    def _post_process_claims(self, data: Dict) -> Dict:
        """
        Post-process extracted claims to fix formatting and field mapping
        Cleanup and deduplicate claims using math-driven quality scores.
        """
        if "claims" not in data or not data["claims"]:
            return data
            
        # Status Mapping
        status_map = {
            'C': 'Closed', 'CL': 'Closed', 'CLOSED': 'Closed',
            'O': 'Open', 'OP': 'Open', 'OPEN': 'Open',
            'R': 'Reopened', 'RC': 'Reopened', 'REOP': 'Reopened', 'REOPENED': 'Reopened'
        }
        
        # Numeric fields to clean
        num_fields = [
            "medical_paid", "medical_reserve", "indemnity_paid", "indemnity_reserve",
            "expense_paid", "expense_reserve", "recovery", "deductible", "total_incurred"
        ]
        
        seen_claim_numbers = {} # claim_number -> (claim_obj, quality_score)
        
        for claim in data["claims"]:
            claim_num = str(claim.get("claim_number", "")).strip()
            if not claim_num:
                continue
                
            # 1. Normalize Status
            raw_status = str(claim.get("status", "")).upper().strip()
            claim["status"] = status_map.get(raw_status, raw_status)
            
            # 2. Normalize Injury Type (MED/COMP)
            raw_type = str(claim.get("injury_type", "")).upper()
            if any(x in raw_type for x in ["COMP", "TTD", "TPD", "PPD", "INDEMNITY", "INDEM"]):
                claim["injury_type"] = "COMP"
            elif any(x in raw_type for x in ["MED", "MEDICAL"]):
                claim["injury_type"] = "MED"
            
            # 3. Numeric cleanup
            for field in num_fields:
                val = claim.get(field)
                if isinstance(val, str):
                    clean_val = re.sub(r'[^\d.]', '', val)
                    try:
                        claim[field] = float(clean_val) if clean_val else 0.0
                    except:
                        claim[field] = 0.0
                elif val is None:
                    claim[field] = 0.0

            # 3b. MED-only Guardrail: If injury_type is MED, Indemnity MUST be 0
            if claim.get("injury_type") == "MED":
                claim["indemnity_paid"] = 0.0
                claim["indemnity_reserve"] = 0.0

            # 3c. Extract Claim Year from Injury Date
            injury_date = str(claim.get("injury_date_time", "")).strip()
            claim["claim_year"] = None
            if injury_date:
                # Expecting YYYY-MM-DD or MM/DD/YYYY
                # Try finding 4 digits that start with 19 or 20
                match = re.search(r'(?:19|20)\d{2}', injury_date)
                if match:
                    try:
                        claim["claim_year"] = int(match.group(0))
                    except:
                        pass

            # 4. Calculate Quality Score (Checksum Match)
            med_inc = claim.get("medical_paid", 0) + claim.get("medical_reserve", 0)
            indem_inc = claim.get("indemnity_paid", 0) + claim.get("indemnity_reserve", 0)
            exp_inc = claim.get("expense_paid", 0) + claim.get("expense_reserve", 0)
            
            # recovery is usually a subtraction from the total
            recovery = claim.get("recovery", 0)
            calc_sum = med_inc + indem_inc + exp_inc
            calc_net = calc_sum - recovery
            
            reported_total = claim.get("total_incurred", 0)
            
            # Check if calc_sum matches perfectly or calc_net matches
            quality_score = 0.5
            err_net = abs(calc_net - reported_total)
            err_gross = abs(calc_sum - reported_total)
            
            if err_net < 1.0 or err_gross < 1.0:
                quality_score = 1.0
            
            claim["math_valid"] = (quality_score == 1.0)
            claim["math_diff"] = round(min(err_net, err_gross), 2)
            
            # Specific fix for "Reserve vs Incurred" (Boyce Case)
            # If we picked up a 'Gross' number as a reserve, the math will fail.
            # We try adjusting the reserve downwards if a recovery exists.
            if quality_score < 1.0 and recovery > 0:
                for cat in ["medical", "indemnity", "expense"]:
                    r_f = f"{cat}_reserve"
                    if claim.get(r_f) > recovery:
                        # Try subtracting recovery from this reserve
                        test_res = claim.get(r_f) - recovery
                        test_sum = (calc_sum - claim.get(r_f) + test_res) - recovery
                        if abs(test_sum - reported_total) < 1.0:
                            claim[r_f] = test_res
                            quality_score = 1.0
                            break

            # AmTrust specific fix for Watson/Duarte cases where numbers might be duplicated
            for cat in ["medical", "indemnity", "expense"]:
                p_f = f"{cat}_paid"
                r_f = f"{cat}_reserve"
                if claim.get(p_f) > 0 and claim.get(p_f) == claim.get(r_f):
                    if quality_score < 1.0:
                        alt_sum = (calc_sum - claim.get(r_f)) - recovery
                        if abs(alt_sum - reported_total) < 1.0:
                            claim[r_f] = 0.0
                            quality_score = 1.0
            
            # 5. Name Normalization (Last, First)
            # If name is "First Last", convert to "Last, First"
            raw_name = str(claim.get("employee_name", "")).strip()
            if raw_name and "," not in raw_name:
                name_parts = raw_name.split()
                if len(name_parts) >= 2:
                    # Heuristic: Assume last word is surname for simple cases
                    # "John Smith" -> "Smith, John"
                    # "John M. Smith" -> "Smith, John M."
                    last = name_parts[-1]
                    first = " ".join(name_parts[:-1])
                    claim["employee_name"] = f"{last}, {first}"
            
            # 7. Deduplicate using Seen dictionary
            claim_num = str(claim.get("claim_number", "unknown")).strip()
            if claim_num != "unknown":
                if claim_num not in seen_claim_numbers:
                    seen_claim_numbers[claim_num] = (claim, quality_score)
                else:
                    existing_claim, old_score = seen_claim_numbers[claim_num]
                    # If this one has better math, keep it
                    if quality_score > old_score:
                        seen_claim_numbers[claim_num] = (claim, quality_score)
                    elif quality_score == old_score:
                        # If scores equal, keep the one with more data
                        new_count = sum(1 for f in num_fields if claim.get(f, 0) > 0)
                        old_count = sum(1 for f in num_fields if existing_claim.get(f, 0) > 0)
                        if new_count > old_count:
                            seen_claim_numbers[claim_num] = (claim, quality_score)
            
        # Rebuild claims list and apply global filters
        final_claims = []
        for claim, quality_score in seen_claim_numbers.values():
            # PHANTOM FILTER: Remove calibration placeholders
            name_raw = str(claim.get("employee_name", "")).lower().strip()
            name_clean = name_raw.replace(",", "").replace(".", "").strip()
            
            # Catch calibration examples and phantom placeholders
            phantom_names = [
                "john smith", "doe john", "john doe", "smith jane", "jane smith", 
                "alice johnson", "johnson alice", "michael johnson", "johnson michael",
                "duarte milian", "milian duarte", "linda johnson", "boyce michael", 
                "michael boyce", "glenn watson", "watson glenn"
            ]
            if name_clean in phantom_names:
                print(f"      üóëÔ∏è  Filtering phantom calibration claim: {claim.get('employee_name')}")
                continue
                
            if any(f in name_raw for f in ["placeholder", "test person"]):
                continue
                
            # GLOBAL NAME NORMALIZATION (Ensure Last, First)
            raw_name = str(claim.get("employee_name", "")).strip()
            if raw_name and "," not in raw_name:
                name_parts = raw_name.split()
                if len(name_parts) >= 2:
                    last = name_parts[-1]
                    first = " ".join(name_parts[:-1])
                    claim["employee_name"] = f"{last}, {first}"
                    
            final_claims.append(claim)
            
        data["claims"] = final_claims
        
        # Log final validation results
        for claim in data["claims"]:
            is_valid, errors = self._validate_financial_data(claim)
            if not is_valid:
                print(f"   ‚ö†Ô∏è  Financial validation warnings for {claim.get('claim_number')}:")
                for error in errors:
                    print(f"      - {error}")
                    
        return data
    
    def _validate_financial_data(self, claim: Dict) -> Tuple[bool, List[str]]:
        """
        Validate financial calculations for a claim
        Returns: (is_valid, list_of_errors)
        """
        errors = []
        tolerance = 0.02  # Allow $0.02 tolerance for rounding
        
        # Get values
        medical_paid = claim.get('medical_paid', 0.0) or 0.0
        medical_reserve = claim.get('medical_reserve', 0.0) or 0.0
        indemnity_paid = claim.get('indemnity_paid', 0.0) or 0.0
        indemnity_reserve = claim.get('indemnity_reserve', 0.0) or 0.0
        expense_paid = claim.get('expense_paid', 0.0) or 0.0
        expense_reserve = claim.get('expense_reserve', 0.0) or 0.0
        total_incurred = claim.get('total_incurred', 0.0) or 0.0
        
        # Calculate expected totals
        medical_incurred = medical_paid + medical_reserve
        indemnity_incurred = indemnity_paid + indemnity_reserve
        expense_incurred = expense_paid + expense_reserve
        
        calculated_total = medical_incurred + indemnity_incurred + expense_incurred
        
        # Validate total incurred
        if abs(calculated_total - total_incurred) > tolerance:
            errors.append(
                f"Total mismatch: calculated ${calculated_total:.2f} != reported ${total_incurred:.2f}"
            )
        
        # Check for negative values
        for field in ['medical_paid', 'medical_reserve', 'indemnity_paid', 
                      'indemnity_reserve', 'expense_paid', 'expense_reserve', 'total_incurred']:
            value = claim.get(field, 0.0) or 0.0
            if value < 0:
                errors.append(f"{field} is negative: ${value:.2f}")
        
        is_valid = len(errors) == 0
        return is_valid, errors
    
    
    def _extract_missing_claims_by_number(self, all_text: str, existing_data: Dict, missing_claim_numbers: List[str], is_correction: bool = False) -> Dict:
        """
        Retry extraction for specific missing claim numbers identified by AI
        OR retry if math validation failed (is_correction=True).
        """
        if not missing_claim_numbers:
            return {"claims": []}
            
        retry_type = "CORRECTION" if is_correction else "RECOVERY"
        print(f"   [{retry_type}] Attempting matching for: {', '.join(missing_claim_numbers)}")
        
        correction_note = ""
        if is_correction:
            correction_note = """
‚ö†Ô∏è MATH VALIDATION FAILED for these claims in the previous pass. 
Common causes:
1. Swapped Medical and Indemnity columns.
2. Missed Recovery/Subro column (often the rightmost column).
3. Confusing Reserves with Paid amounts in multi-row layouts.

RE-EXAMINE the column headers and row labels for these specific IDs and ensure the math balances:
Medical(Paid+Res) + Indemnity(Paid+Res) + Expense(Paid+Res) - Recovery == Total Incurred.
"""

        retry_prompt = f"""You are an expert insurance data extractor.
{correction_note}

Your Task: Extract COMPLETE data for ONLY these specific claim numbers:
{', '.join(missing_claim_numbers)}

Return a JSON object with this structure:
{{
  "claims": [
    {{
      "employee_name": "full name",
      "claim_number": "exact claim number",
      "injury_date_time": "YYYY-MM-DD",
      "status": "Open/Closed/Reopened",
      "injury_description": "description",
      "body_part": "body part or null",
      "injury_type": "MED or COMP",
      "claim_class": "class code",
      "medical_paid": "string",
      "medical_reserve": "string",
      "indemnity_paid": "string",
      "indemnity_reserve": "string",
      "expense_paid": "string",
      "expense_reserve": "string",
      "recovery": "string",
      "deductible": "string",
      "total_incurred": "string"
    }}
  ]
}}

STRICT RULES:
1. DO NOT include any claims NOT in the list above.
2. Ensure math balances perfectly.
3. Check if 'Total Incurred' includes or excludes 'Recovery'.

TEXT TO ANALYZE:
{all_text}

Return ONLY the JSON."""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": retry_prompt}],
                response_format={"type": "json_object"},
                max_tokens=8000,
                temperature=0.0
            )
            
            retry_data = json.loads(response.choices[0].message.content)
            if "claims" in retry_data:
                retry_data = self._post_process_claims(retry_data)
                return retry_data
            return {"claims": []}
        except Exception as e:
            print(f"      ‚ö†Ô∏è  Extraction retry failed: {e}")
            return {"claims": []}
    
    def _extract_single_claim(self, all_text: str, target_claim_number: str) -> Dict:
        """
        Extract only a specific claim by claim number
        """
        prompt = f"""You are extracting structured data from an insurance document.

This document may contain MULTIPLE claims, but you should extract ONLY the claim with number: {target_claim_number}

Return a JSON object with this structure:

{{
  "employee_name": "full claimant name",
  "claim_number": "{target_claim_number}",
  "injury_date_time": "YYYY-MM-DD",
  "claim_year": 2020,
  "status": "Open/Closed/REOP",
  "injury_description": "cause of injury",
  "body_part": "injured body part",
  "injury_type": "COMP/MEDI/etc",
  "claim_class": "class code and description",
  "medical_paid": 0.0,
  "medical_reserve": 0.0,
  "indemnity_paid": 0.0,
  "indemnity_reserve": 0.0,
  "expense_paid": 0.0,
  "expense_reserve": 0.0,
  "recovery": 0.0,
  "deductible": 0.0,
  "total_incurred": 0.0
}}

RULES:
1. Find the claim with number {target_claim_number}
2. Extract ONLY that claim's data
3. Ignore all other claims in the document
4. Status codes: C=Closed, O=Open, REOP=Reopened
5. Remove $ and commas from amounts

TEXT TO ANALYZE:
{all_text}

Return ONLY the JSON object for claim {target_claim_number}."""

        try:
            response = self.client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": prompt
                }],
                response_format={"type": "json_object"},
                max_tokens=8000,
                temperature=0.1
            )
            
            response_text = response.choices[0].message.content
            data = json.loads(response_text)
            
            # Wrap in 'claims' list for post-processing consistency
            wrapped_data = {"claims": [data]}
            processed_data = self._post_process_claims(wrapped_data)
            
            # Extract the single processed claim back
            if processed_data.get("claims"):
                data = processed_data["claims"][0]
                
            print(f"‚úì Extracted and processed claim #{target_claim_number}")
            return data
            
        except Exception as e:
            print(f"‚ùå Error extracting schema: {e}")
            return {}
    
    def validate_extraction(self, data: Dict, original_text: str) -> Dict:
        """
        Validate that all claims in the text were extracted
        """
        print(f"\nüîç Validating extraction...")
        
        # Find all claim numbers mentioned in text
        claim_numbers_in_text = set()
        
        # Pattern 1: "Claim# 20677" or "Claim #20677"
        for match in re.finditer(r'Claim#?\s*(\d+)', original_text, re.IGNORECASE):
            claim_numbers_in_text.add(match.group(1))
            
        # Get claim numbers from extracted data
        if "claims" in data:
            # Multi-claim format
            claim_numbers_extracted = {
                str(claim.get("claim_number", "")) 
                for claim in data.get("claims", [])
                if claim.get("claim_number")
            }
        else:
            # Single claim format
            claim_num = data.get("claim_number")
            claim_numbers_extracted = {str(claim_num)} if claim_num else set()
        
        # Check for missing claims
        missing_claims = claim_numbers_in_text - claim_numbers_extracted
        extra_claims = claim_numbers_extracted - claim_numbers_in_text
        
        validation_report = {
            "total_in_text": len(claim_numbers_in_text),
            "total_extracted": len(claim_numbers_extracted),
            "missing_claims": list(missing_claims),
            "extra_claims": list(extra_claims),
            "is_complete": len(missing_claims) == 0
        }
        
        # Print report
        print(f"   Claims in text: {len(claim_numbers_in_text)}")
        print(f"   Claims extracted: {len(claim_numbers_extracted)}")
        
        if missing_claims:
            print(f"   ‚ö†Ô∏è  MISSING: {', '.join(missing_claims)}")
        
        if extra_claims:
            print(f"   ‚ö†Ô∏è  EXTRA: {', '.join(extra_claims)}")
        
        if validation_report["is_complete"]:
            print(f"   ‚úì Extraction is COMPLETE")
        else:
            print(f"   ‚ùå Extraction is INCOMPLETE")
        
        return validation_report

    
    def process_pdf_with_verification(self, pdf_path: str, target_claim_number: Optional[str] = None) -> Dict:
        """
        Complete pipeline with verification steps
        Uses PyMuPDF + Tesseract for text extraction
        Returns all extracted data for user verification
        """
        print(f"\n{'='*60}")
        print(f"üöÄ PROCESSING: {os.path.basename(pdf_path)}")
        print(f"{'='*60}")
        
        # Create session output directory with high precision and filename for uniqueness
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:20] # Add microseconds
        file_slug = os.path.basename(pdf_path).replace(" ", "_").replace(".", "_")[:20]
        session_id = f"{timestamp}_{file_slug}"
        session_dir = self.output_dir / f"extraction_{session_id}"
        session_dir.mkdir(parents=True, exist_ok=True)
        
        # Step 1: Extract text from PDF using PyMuPDF + Tesseract
        all_text, pages_metadata = self.extract_text_from_pdf(pdf_path)
        
        # Prepare page data for compatibility
        pages_data = pages_metadata
        
        # Save combined text for verification
        text_file = session_dir / "extracted_text.txt"
        with open(text_file, 'w', encoding='utf-8') as f:
            f.write(all_text)
        print(f"\n‚úì Combined text saved: {text_file}")
        
        # Step 2: Extract schema from combined text
        print(f"\n{'='*60}")
        print(f"üìã SCHEMA EXTRACTION")
        print(f"{'='*60}")
        
        schema_data = self.extract_schema_from_text(all_text, target_claim_number)
        
        # Validate extraction
        validation = self.validate_extraction(schema_data, all_text)
        
        # Print metadata to terminal (not saved to JSON)
        print(f"\n{'='*60}")
        print(f"üìä EXTRACTION METADATA")
        print(f"{'='*60}")
        print(f"Session ID: {session_id}")
        print(f"Source File: {os.path.basename(pdf_path)}")
        print(f"Total Pages: {len(pages_metadata)}")
        print(f"Extraction Method: pymupdf-tesseract-enhanced")
        print(f"Validation: {validation['total_extracted']} claims extracted, {len(validation['missing_claims'])} missing")
        
        # Add minimal metadata to JSON (without pages_metadata)
        extraction_metadata = {
            "extraction_date": datetime.now().isoformat(),
            "method": "pymupdf-tesseract-enhanced",
            "num_pages": len(pages_metadata),
            "source_file": os.path.basename(pdf_path),
            "session_id": session_id,
            "target_claim": target_claim_number
        }
        schema_data['extraction_metadata'] = extraction_metadata
        
        # Create analysis.json with metadata and policy details (without claims)
        analysis_data = {
            "extraction_metadata": extraction_metadata,
            "report_date": schema_data.get("report_date"),
            "policy_number": schema_data.get("policy_number"),
            "insured_name": schema_data.get("insured_name"),
            "policy_period": schema_data.get("policy_period"),
            "total_claims": len(schema_data.get("claims", []))
        }
        
        # Save analysis.json (metadata only, no claims data)
        analysis_file = session_dir / "analysis.json"
        with open(analysis_file, 'w', encoding='utf-8') as f:
            json.dump(analysis_data, f, indent=2, ensure_ascii=False)
        print(f"‚úì Analysis saved: {analysis_file}")
        
        # Save schema (ONLY claims array - clean output)
        claims_only = {
            "claims": schema_data.get("claims", [])
        }
        schema_file = session_dir / "extracted_schema.json"
        with open(schema_file, 'w', encoding='utf-8') as f:
            json.dump(claims_only, f, indent=2, ensure_ascii=False)
        print(f"‚úì Schema saved: {schema_file}")
        
        # Step 3: Prepare verification package (for internal use only)
        # Note: verification_data contains full schema_data for internal processing
        # But extracted_schema.json file only contains claims array
        verification_data = {
            "session_id": session_id,
            "session_dir": str(session_dir),
            "source_pdf": pdf_path,
            "pages": pages_data,
            "combined_text": all_text,
            "combined_text_file": str(text_file),
            "extracted_schema": claims_only,  # Use claims_only (no extra metadata)
            "schema_file": str(schema_file),
            "summary": {
                "total_pages": len(pages_metadata),
                "scanned_pages": sum(1 for p in pages_metadata if p.get('is_scanned', False)),
                "avg_confidence": sum(p.get('confidence', 0.0) for p in pages_metadata) / len(pages_metadata) if pages_metadata else 0.0,
                "extraction_methods": [p.get('extraction_method', 'unknown') for p in pages_metadata]
            }
        }
        
        # Save verification package
        verification_file = session_dir / "verification_package.json"
        with open(verification_file, 'w', encoding='utf-8') as f:
            json.dump(verification_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\n{'='*60}")
        print(f"‚úÖ EXTRACTION COMPLETE")
        print(f"{'='*60}")
        print(f"Session: {session_id}")
        print(f"Output: {session_dir}")
        print(f"\nFiles created:")
        print(f"  - extracted_text.txt (combined text)")
        print(f"  - extracted_schema.json (structured data)")
        print(f"  - verification_package.json (full package)")
        print(f"\nExtraction Summary:")
        print(f"  - Total pages: {verification_data['summary']['total_pages']}")
        print(f"  - Scanned pages: {verification_data['summary']['scanned_pages']}")
        print(f"  - Avg confidence: {verification_data['summary']['avg_confidence']:.2%}")
        
        return verification_data


def main():
    """Main function"""
    import sys
    from dotenv import load_dotenv
    
    # Load environment variables
    load_dotenv()
    
    # Check for API key
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("\n‚ùå OPENAI_API_KEY not set!")
        print("Set it with: export OPENAI_API_KEY='sk-...'")
        print("Get your key from: https://platform.openai.com/api-keys")
        return
    
    extractor = EnhancedInsuranceExtractor(api_key)
    
    # Get PDF path
    if len(sys.argv) > 1:
        pdf_path = sys.argv[1]
        target_claim = sys.argv[2] if len(sys.argv) > 2 else None
    else:
        upload_dir = "/mnt/user-data/uploads"
        if os.path.exists(upload_dir):
            pdfs = [f for f in os.listdir(upload_dir) if f.lower().endswith('.pdf')]
            if pdfs:
                pdf_path = os.path.join(upload_dir, pdfs[0])
                print(f"Found PDF: {pdf_path}")
                target_claim = None
            else:
                print("Usage: python enhanced_extractor.py <pdf_path> [claim_number]")
                return
        else:
            print("Usage: python enhanced_extractor.py <pdf_path> [claim_number]")
            return
    
    if not os.path.exists(pdf_path):
        print(f"Error: File not found: {pdf_path}")
        return
    
    # Process with verification
    result = extractor.process_pdf_with_verification(pdf_path, target_claim)
    
    if "error" in result:
        print(f"\n‚ùå Error: {result['error']}")
        return
    
    # Display summary
    print("\n" + "="*60)
    print("EXTRACTION SUMMARY")
    print("="*60)
    print(f"Pages processed: {result['summary']['total_pages']}")
    print(f"Scanned pages: {result['summary']['scanned_pages']}")
    print(f"Avg confidence: {result['summary']['avg_confidence']:.2%}")
    print(f"\nOrientations: {', '.join(result['summary'].get('orientations', []))}")
    
    print("\n" + "="*60)
    print("EXTRACTED SCHEMA")
    print("="*60)
    print(json.dumps(result['extracted_schema'], indent=2, default=str))
    
    print("\n" + "="*60)
    print(f"‚úì All files saved to: {result['session_dir']}")
    print("="*60)


if __name__ == "__main__":
    main()